<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- TODO change title -->
    <title>
      Robotic Constrained Imitation Learning for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery
    </title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.7.0/font/bootstrap-icons.css" rel="stylesheet">
    <link rel="stylesheet" href="assets/css/main.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1XJ0NHR591"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-1XJ0NHR591');
    </script>

    <meta property="og:url"           content="https://haraduka.github.io/fls-imitation" />
    <meta property="og:type"          content="website" />
    <meta property="og:title"         content="
      Robotic Constrained Imitation Learning for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery
    " />
    <meta property="og:description"   content="
    In this study, we present an implementation strategy for a robot that performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via imitation learning, aimed at the development of an autonomous robot for laparoscopic surgery.
    Robotic laparoscopic surgery presents two main challenges: (1) the need to manipulate forceps using ports established on the body surface as fulcrums, and (2) difficulty in perceiving depth information when working with a monocular camera that displays its images on a monitor.
    Especially, regarding issue (2), most prior research has assumed the availability of depth images or models of a target to be operated on.
    Therefore, in this study, we achieve more accurate imitation learning with only monocular images by extracting motion constraints from one exemplary motion of skilled operators, collecting data based on these constraints, and conducting imitation learning based on the collected data.
    We implemented an overall system using two Franka Emika Panda Robot Arms and validated its effectiveness.
    " />
    <meta property="og:image" content="https://haraduka.github.io/fls-imitation/assets/img/banner.png" />
  </head>
  <body>
    <div class="container-fluid">
      <div class="row">
        <div class="col-lg-8 offset-lg-2 col-md-12">

          <div class="text-center">
            <h1 class="mt-5"><b>Robotic Constrained Imitation Learning</b></h1>
            <h2 class="mt-3"><b>for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery</b></h1>
            <ul class="list-inline mt-4">
              <li class="list-inline-item"><a href="https://haraduka.github.io" target="_blank">Kento Kawaharazuka</a></li>
              <li class="list-inline-item ml-4">Kei Okada</li>
              <li class="list-inline-item ml-4">Masayuki Inaba</li>
              <li class="mt-2">
                JSK Robotics Laboratory, The University of Tokyo, Japan
              </li>
            </ul>
            <ul class="list-inline mt-4">
              <li class="list-inline-item">
                <a href="https://arxiv.org/abs/2405.03440" target="_blank">Paper</a>
              </li>
              <li class="list-inline-item ml-4">
                <a href="https://youtu.be/cg7bFTj_hPo" target="_blank">Video</a>
              </li>
            </ul>
          </div>

          <div class="row mt-4">
            <div class="col-lg-10 offset-lg-1">
              <p>
    In this study, we present an implementation strategy for a robot that performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via imitation learning, aimed at the development of an autonomous robot for laparoscopic surgery.
    Robotic laparoscopic surgery presents two main challenges: (1) the need to manipulate forceps using ports established on the body surface as fulcrums, and (2) difficulty in perceiving depth information when working with a monocular camera that displays its images on a monitor.
    Especially, regarding issue (2), most prior research has assumed the availability of depth images or models of a target to be operated on.
    Therefore, in this study, we achieve more accurate imitation learning with only monocular images by extracting motion constraints from one exemplary motion of skilled operators, collecting data based on these constraints, and conducting imitation learning based on the collected data.
    We implemented an overall system using two Franka Emika Panda Robot Arms and validated its effectiveness.
              </p>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Fundamentals of Laparoscopic Surgery for Robots</h4>
            <p>
            Regarding peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) for robots using imitation learning, we handle two main problems: (1) the forceps are constrained by laparoscopic ports and (2) only RGB information can be obtained from a monocular endoscope.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-8 offset-lg-2">
                <img src="assets/img/concept.png" class="img-fluid">
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>The Setup for Robotic Peg Transfer Tasks in Fundamentals of Laparoscopic Surgery</h4>
            <p>
            Franka Emika Panda Robot Arms are operated by Touch Haptic Device (3D Systems Corp.). Maryland Dissectors are controlled via Hand Adapter and Parallel Gripper to transfer Rubber Object on Peg Board.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/fls-setup.png" class="img-fluid">
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Constrained Inverse Kinematics for Laparoscopic Surgery</h4>
            <p>
            The configuration to control forceps considering the constraint by a laparoscopic port. The left figure shows a geometric model of robot arms, and the right figure shows the kinematics of forceps with a virtual linear joint from the tip of the hand that overlaps with the long axis of the forceps.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-8 offset-lg-2">
                <img src="assets/img/fls-ik.png" class="img-fluid">
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Constrained Data Collection and Imitation Learning</h4>
            <p>
  In order to generate robot motions by imitation learning, we collect human teaching data using haptic devices.
  However, it is challenging to manipulate the forceps successfully because it is difficult to obtain the depth information.
  Therefore, we extract constraints on the movement of the forceps from the trajectory of a single slow and accurate exemplary demonstration, and incorporate them to improve the quality of the teaching data and the accuracy of imitation learning.
  The procedure is (1) Describe a phase transition condition, (2) Extract motion constraints pertaining to each phase from a single exemplary demonstration, (3) Collect data by human teaching with force feedback based on the constraints, (4) Execute imitation learning based on the collected data.
  Note that this method is not limited to the FLS task, but can be applied to any task by changing the phase transition condition and motion constraint extraction.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-8 offset-lg-2">
                <img src="assets/img/fls-learning.png" class="img-fluid">
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Experiments</h4>
            <div class="mt-4">
              <h5>One Exemplary Demonstration of Peg transfer</h4>
              <div align="center" class="row mt-4">
                <div class="col-lg-10 offset-lg-1">
                  <img src="assets/img/complete-exp1.gif" class="img-fluid">
                </div>
              </div>
              <div align="center" class="row mt-4">
                <div class="col-lg-10 offset-lg-1">
                  <img src="assets/img/complete-exp2.gif" class="img-fluid">
                </div>
              </div>
            </div>
            <div class="mt-4">
              <h5>Contrained Data Collection vs. Normal Data Collection</h4>
              <p>
              The average variance for Constrained is 5.65 for the left arm and 2.74 for the right arm, whereas for Normal, it is 6.56 for the left arm and 4.78 for the right arm.
              Imposing minimum and maximum constraints on the z-directional movement allows for stable data collection.
              </p>
              <div align="center" class="row mt-4">
                <div class="col-lg-10 offset-lg-1">
                  <img src="assets/img/collect.gif" class="img-fluid">
                </div>
              </div>
            </div>
            <div class="mt-4">
              <h5>Imitation Learning Result with Constrained Data Collection</h4>
              <div align="center" class="row mt-4">
                <div class="col-lg-10 offset-lg-1">
                  <img src="assets/img/task-exp.gif" class="img-fluid">
                </div>
              </div>
            </div>
            <div class="mt-4">
              <h5>Comparison of Success Rates</h4>
              <div align="center" class="row mt-4">
                <div class="col-lg-8 offset-lg-2">
                  <img src="assets/img/task-graph.png" class="img-fluid">
                </div>
              </div>
            </div>
            <div class="mt-4">
              <h5>Comparison of the Trajectories of LSTM's Latent Space</h4>
              <div align="center" class="row mt-4">
                <div class="col-lg-10 offset-lg-1">
                  <img src="assets/img/task-pca.png" class="img-fluid">
                </div>
              </div>
            </div>
          </div>

          <!-- bibtex -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 id="bibtex">Bibtex</h4>
            <pre>
@inproceedings{kawaharazuka2024fls,
  author={K. Kawaharazuka and K. Okada and M. Inaba},
  title={{Robotic Constrained Imitation Learning for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery}},
  booktitle="2024 IEEE International Conference on Robotics and Automation",
  year=2024,
}
            </pre>
          </div>

          <!-- contact -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1 mb-5">
            <h4 id="contact">Contact</h4>
            <p>
            If you have any questions, please feel free to contact
            <a href="https://haraduka.github.io" target="_blank">Kento Kawaharazuka</a>.
            </p>
          </div>

        </div>
      </div>
    </div>
  </body>
</html>
